---
title: "Week 3 Tutorial"
subtitle: "Applied Analytical Statistics 2025/26"
authors: Paul Röttger and Mikhail Korneev
output:
  html_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This tutorial focuses on the application of hypothesis testing to real-world data. It covers one-sample t-test, two-sample t-test, chi-squared test, and paired t-test. For all questions regarding the materials, please contact Mikhail Korneev at mikhail.korneev@reuben.ox.ac.uk. 

In this tutorial, we will be working with an extended version of paragraph data from last two weeks. For every human-written paragraph, there is an AI-assisted paragraph version. Paragraph authorship is indicated by the paragraph_type column ("writer" or "model").

First, download the new version of the dataset from GitHub. 

```{r load data, message = FALSE, warning = FALSE, }

library(readr)

df <- read_csv(
  "https://raw.githubusercontent.com/paul-rottger/aas-2026-public/refs/heads/main/week3/tutorial/week3_tutorial_sample.csv"
)

```

Let's familiarize ourselves with the new dataset. 

```{r data description, message = FALSE} 

library(dplyr)
library(tidyr)

# A quick look at the variables
head(df, 10)

# Let's count the number of writers, raters, and paragraphs (by type)
df %>%
  summarise(
    `unique raters` = n_distinct(rater_id),
    writers = n_distinct(writer_id),                   
    paragraphs = list(
      count(pick(everything()), paragraph_type) %>%
        pivot_wider(
          names_from  = paragraph_type,
          values_from = n,
          values_fill = 0,
          names_prefix = "paragraphs "
        )
    )
  ) %>%
  unnest(paragraphs)

```

### Testing a single mean (one-sample t-test) 

From the week 2 lecture, we know that the sampling distribution for i.i.d. samples is well approximated by a normal distribution. Using sample-based estimates, in the tutorial for week 2, we constructed **Z-based confidence intervals** for population parameters.

In practice, Z-based inference requires strong assumptions about the population standard deviation, which is usually unknown. Instead, it is common to use the **t-distribution**, which accounts for the additional uncertainty introduced by estimating the population standard deviation from the sample. An alternative to the analytical approach is the computational approach that uses **bootstrap methods**.

**Practice:**

Consider the function below, which returns a p-value for a **two-sided one-sample t-test**. 

```{r 1-sample-t theory}

# one-sample t-test (two_tailed)
one_sample_t_pvalue <- function(x, mu) {
  n  <- length(x)
  x_bar  <- mean(x)
  s  <- sd(x)
  
  t_stat <- (x_bar - mu) / (s / sqrt(n))
  df <- n - 1
  
  p_value <- 2 * pt(-abs(t_stat), df)
  
  # decision message (alpha = 0.05)
  if (p_value < 0.05) {
    cat("Reject the null hypothesis, p-value =", p_value, "\n")
  } else {
    cat("Fail to reject the null hypothesis, p-value =", p_value, "\n")
  }

}

# example usage: 
#one_sample_t_pvalue(df$writer_knowledge, 50)

```

What is the interpretation of the **t-statistic** computed by this function? How should the resulting **p-value** be interpreted?

```

The t statistic is the standardized distance between the sample mean and the hypothesized population mean, measured in units of the sample’s standard error. Larger absolute t values indicate stronger evidence that the sample mean differs from the hypothesized mean.

The p-value is the probability of observing a t statistic at least as extreme as the one obtained, assuming the null hypothesis is true. A small p-value indicates that the observed difference is unlikely to have occurred by chance under the null hypothesis.

```

**Practice:**

Choose one continuous variable from the updated dataset. Test whether the true population mean is equal to 50 (H0: mu = 50) or is a different from 50 (H1: mu != 50) at the 95% confidence level separately for human-written paragraphs and AI-written paragraphs. 

In R, a t-test can be performed using the `t.test()` function from the `stats` package. Conduct the test using the `one_sample_t_pvalue()` manual function and confirm the results with `t.test()`. 


```{r 1-sample-t practice 1} 
library(stats)

# 1. Splitting the sample into human- and AI-written paragraphs
df_human <- subset(df, df$paragraph_type == "writer")
df_AI <- subset(df, df$paragraph_type == "model")

# 2. One sample t-test for a 95% CI using probability theory 

# for writer_confidence for human-written paragraphs
one_sample_t_pvalue(df_human$writer_confidence, mu = 50) # manual function

t.test(df_human$writer_confidence, mu = 50, conf.level = 0.95) # t.test()

# for writer_confidence for AI-written paragraphs
one_sample_t_pvalue(df_AI$writer_confidence, mu = 50) # manual function 

t.test(df_AI$writer_confidence, mu = 50, conf.level = 0.95) # t.test()

```

**Practice:**

While the t-distribution accounts for the uncertainty introduced by estimating the population standard deviation from the sample, the t-test may perform poorly when the population variance is too complex or when the sample size is small. From the previous tutorial, we know that these issues can be addressed using **bootstrap methods**.

Consider a manual **bootstrap t-test function** below: 

```{r 1-sample-t theory bootstrap} 
# 1-sample bootstrap t-test
one_sample_bootstrap_pvalue <- function(x, mu, B) {
  
  n <- length(x)
  x_bar <- mean(x)
  s <- sd(x)
  
  # observed t-statistic
  t_obs <- (x_bar - mu) / (s / sqrt(n))
  
  # shift data to satisfy H0
  x_null <- x - x_bar + mu
  
  # bootstrap t-statistics under H0
  boot_t <- replicate(
    B,
    {
      samp <- sample(x_null, n, replace = TRUE)
      (mean(samp) - mu) / (sd(samp) / sqrt(n))
    }
  )
  
  # two-sided p-value using t-statistics
  p_value <- mean(abs(boot_t) >= abs(t_obs))
  
  # decision message (alpha = 0.05)
  if (p_value < 0.05) {
    cat("Reject the null hypothesis, p-value =", p_value, "\n")
  } else {
    cat("Fail to reject the null hypothesis, p-value =", p_value, "\n")
  }
  
}

```

Use the manual `one_sample_bootstrap_pvalue()` function to re-run the t-test from the previous exercise. Does the bootstrap method produce different results?

Confirm the results using a `boot.t.test()` function from the `MKinfer` package

```{r 1-sample-t practice 2}
set.seed(123)

# One sample t-test for a 95% CI using bootstrap
library(MKinfer)

# for writer_confidence for human-written paragraphs
one_sample_bootstrap_pvalue(df_human$writer_confidence, 50, 1000) # manual function 

boot.t.test(df_human$writer_confidence, mu = 50, conf.level = 0.95, 
            R = 1000) # boot.t.test()

# for writer_confidence for AI-written paragraphs
one_sample_bootstrap_pvalue(df_AI$writer_confidence, 50, 1000) # manual function 

boot.t.test(df_AI$writer_confidence, mu = 50, conf.level = 0.95, 
            R = 1000) # boot.t.test()

```

### Testing for a difference in means (two-sample t-test) 

One of the most common applications of the t-test is testing for a **difference in sample means**.

Suppose we want to test whether there are systematic differences in the originality between human-written paragraphs and AI-written paragraphs.

- What is the null hypothesis?

- What is the alternative hypothesis?

- What property of the t-distribution allows us to move from estimating a single sample mean to estimating the difference in sample means?

```
H0: The mean perceived originality is the same for human-written and AI-written paragraphs.
H1: The mean perceived originality differs between human-written and AI-written paragraphs.

We assume that the sample means for both groups follow t-distributions (because the population variances are unknown and estimated from the data). Under this assumption, the difference between two sample means, when properly standardized, also follows a t-distribution. This allows us to perform inference on the difference in sample means using the t-test.

```

**Practice:**

Let's update the manual t-test function to estimate the difference in sample means. Edit the 1-sample t-test function below to implement a 2-sample t-test. For simplicity, assume equal variance of the sample means. 

```{r 2-sample-t theory} 

# two-sample t-test (two-tailed, equal variances)
two_sample_t_pvalue <- function(x, y) {
  
  n1 <- length(x)
  n2 <- length(y)
  
  x_bar <- mean(x)
  y_bar <- mean(y)
  
  s1 <- sd(x)
  s2 <- sd(y)
  
  # pooled standard deviation
  s_pooled <- sqrt(
    ((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2)
  )
  
  t_stat <- (x_bar - y_bar) / (s_pooled * sqrt(1 / n1 + 1 / n2))
  
  df <- n1 + n2 - 2
  
  p_value <- 2 * pt(-abs(t_stat), df)
  
  # decision message (alpha = 0.05)
  if (p_value < 0.05) {
    cat("Reject the null hypothesis, p-value =", p_value, "\n")
  } else {
    cat("Fail to reject the null hypothesis, p-value =", p_value, "\n")
  }
}

```

**Practice:**

Let’s return to the dataset. In which characteristics would you expect the perceived qualities of human-written and AI-written paragraphs to differ?

Select one continuous variable and:

- Visualise its distribution for human-written and AI-written paragraphs.

- Test the difference in means between human- and AI-written paragraphs using either the `two_sample_t_pvalue()`, confirm the results with the `t.test()` function. 

**Extra:** Estimate the difference in means using **bootstrapping**. Do the bootstrap results differ from those obtained using the standard t-test?

```{r 2-sample-t practice} 

library(ggplot2)

# 1. Visualise the distribution of writer_confidence for the two variables 
ggplot(df, aes(x = writer_confidence, fill = paragraph_type)) +
  geom_density(alpha = 0.4) +
  labs(
    title = "Distribution of writer confidence by paragraph type",
    x = "Writer confidence",
    y = "Density"
  ) +
  theme_minimal()

# 2. Two-sample t-test for a 95% CI using probability theory 

two_sample_t_pvalue(df$writer_confidence[df$paragraph_type == "model"], 
                    df$writer_confidence[df$paragraph_type == "writer"]) # manual function 

t.test(df$writer_confidence ~ df$paragraph_type,
       mu = 0, conf.level = 0.95) # t.test()

# 3. Two-sample t-test for a 95% CI using bootstrap
boot.t.test(df$writer_confidence ~ df$paragraph_type,
       mu = 0, conf.level = 0.95, R = 1000)

```

### Effect size and practical significance (Cohen's d) 

While p-values are informative, they are not the only quantity that matters when interpreting t-test results. Beyond distinguishing statistically significant from non-significant differences, we also want to assess the **size of the effect**.

A common measure of effect size is **Cohen’s d**, which quantifies the difference between sample means in units of the pooled population standard deviation.

```{r Cohen_s d theory} 

# Cohen's d for a two-sample t-test
cohens_d_manual <- function(x, y) {
  
  n1 <- length(x)
  n2 <- length(y)
  
  m1 <- mean(x)
  m2 <- mean(y)
  
  s1 <- sd(x)
  s2 <- sd(y)
  
  #pooled standard deviation 
  s_pooled <- sqrt(
    ((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2)
  )
  
  # d value
  d <- (m1 - m2) / s_pooled
  
  return(d)
}

# Example usage
#cohens_d_manual(df$writer_confidence[df$paragraph_type == "model"], 
#                df$writer_confidence[df$paragraph_type == "writer"])

```

**Practice:**

Returning to the dataset, in which characteristics do human-written and AI-written paragraphs differ the most? Identify two variables for which the difference in means is statistically significant at the 95% confidence level, but where the effect sizes differ. 

**Hint:** Use the `cohens_d_manual()` function and the `two_sample_t_pvalue()` (or the `t.test()`) function as in previous sections to test for differences in means. 

```{r Cohen_s d practice}
# in this solution, I use a cohen_d() function from the effectsize library
library(effectsize)

# making a vector of the variables of interest
variables <- c(
  "writer_confidence",
  "writer_stance",
  "writer_friendliness",
  "writer_openness",
  "paragraph_informativeness"
)

# pre-allocating results data frame
results_df <- data.frame(
  variable = variables,
  p_value  = NA,
  cohens_d = NA
)

# calculating p-values and Cohen's d values for all variables
for (i in seq_along(variables)) {
  
  # t-test
  results_t <- t.test(
    as.formula(paste(variables[i], "~ paragraph_type")),
    data = df,
    mu = 0
  )
  
  results_df$p_value[i] <- round(results_t$p.value, 4) 
  
  # Cohen's d
  results_cohens_d <- cohens_d(
    as.formula(paste(variables[i], "~ paragraph_type")),
    data = df
  ) 
  
  results_df$cohens_d[i] <- round(results_cohens_d$Cohens_d, 4)
}

# displaying the results
results_df

```

Comment on practical vs statistical significance for the two variables. 

```

Writer stance shows a statistically significant difference (p = 0.0001), but the effect size is extremely small (Cohen’s d = 0.06), indicating little to no practical significance. In contrast, paragraph informativeness is both statistically significant (p < 0.001) and has a large effect size (Cohen’s d = 0.74), suggesting a difference that is meaningful in practice as well as statistically reliable.

```

### Testing for a difference in proportions (Chi-squared test) 

Up to this point in the tutorial, we have focused on differences in continuous variables. However, we may also be interested in differences in **categorical variables** between human- and AI-written paragraphs.

To test for differences in proportions, the t-test is not appropriate; instead, we use a **chi-squared test**.

```{r chi_sq theory} 

# chi-squared test for difference in proportions
chisq_pvalue <- function(x, y) {
  
  # contingency table
  obs <- table(x, y)
  
  # expected counts
  row_totals <- rowSums(obs)
  col_totals <- colSums(obs)
  grand_total <- sum(obs)
  
  # Expected counts under independence for each cell
  exp <- outer(row_totals, col_totals) / grand_total
  
  # chi-squared statistic
  chisq_stat <- sum((obs - exp)^2 / exp)
  
  df <- (nrow(obs) - 1) * (ncol(obs) - 1)
  
  # Chi-squared test is not directional => one-tail test
  p_value <- pchisq(chisq_stat, df, lower.tail = FALSE)
  
  # decision message (alpha = 0.05)
  if (p_value < 0.05) {
    cat("Reject the null hypothesis, p-value =", p_value, "\n")
  } else {
    cat("Fail to reject the null hypothesis, p-value =", p_value, "\n")
  }
}

# example of usage 
#chisq_pvalue(df$writer_race, df$paragraph_type)

```

**Practice:** 

In which categorical variables would you expect the perceived qualities of human-written and AI-written paragraphs to differ? Choose one categorical variable and display the contingency table for the selected variable. Then conduct a **chi-squared test** using the `chisq_pvalue()` manual function. Confirm results with a `chisq.test()` function from the `stats` package. 

**Extra:**

Visualise the distribution of the variable across paragraph types using `geom_bar()`.

```{r chi_sq practice}
library(stats)

# Chi-squared test for a difference in proportions for writer_income
chisq_pvalue(df$writer_income, df$paragraph_type) # manual function 
chisq.test(df$writer_income, df$paragraph_type) # chisq.test() function 

# making sure writer_income is in the right order 
df$writer_income <- factor(
  df$writer_income,
  levels = c("Under £15,000", "£15,000-£24,999", "£25,000-£34,999", "£35,000-£49,999", "£50,000-£74,999", "£75,000-£99,999", "£100,000+"))

# contingency table for writer_income
income_table <- table(df$writer_income, df$paragraph_type)
income_table

# distribution of writer_income in each sample 
ggplot(df, aes(x = paragraph_type, fill = writer_income)) +
  geom_bar(position = "fill") +
  labs(
    x = "Paragraph type",
    y = "Proportion",
    fill = "Writer income"
  ) +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_fill_brewer(palette = "RdBu")

```

### Paired t-test

The t-test is a widely used analytical tool in the social sciences. When observations come in **pairs** — for example, repeated measurements on the same unit or matched observations across conditions — it is important to account for this structure.

Paired data allow us to test mean differences **within units**, which removes between-unit variation and results in a more powerful test. In such settings, the appropriate method is the **paired t-test**, rather than an **independent-samples t-test**.

In this final section, we focus on the implementation of the **paired t-test**. Upload the new version of the dataset in which ratings are aggregated across raters for each paragraph. This dataset provides paired data: for each human-written paragraph, there is a corresponding AI-assisted version.

```{r new data, message = FALSE}

library(readr)

df_aggregated <- read_csv(
  "https://raw.githubusercontent.com/paul-rottger/aas-2026-public/refs/heads/main/week3/tutorial/week3_tutorial_sample_aggregated.csv"
)

```

**Practice:**

Using the aggregated dataset, implement a **paired t-test** to compare human-written and AI-assisted versions of the same paragraphs. Choose a variable that you used for the two-sample t-test exercise. 

Next, compare the results of the paired t-test to those obtained from the unpaired t-test on the aggregated data. How do the estimated confidence intervals and p-values differ between the two approaches? 

**Hint:** use `help()` function on `t.test()` to see how to perform a paired t-test. 

```{r paired t-test practice}

# 1. Two-sample t-test for a 95% CI using probability theory 
t.test(df_aggregated$writer_confidence ~ df_aggregated$paragraph_type,
       mu = 0, conf.level = 0.95)

# 2. Paired t-test for a 95% CI using probability theory 

#df_aggregated <- df_aggregated[order(df_aggregated$combinedId, df_aggregated$paragraph_type), ]

t.test(x = df_aggregated$writer_confidence[df_aggregated$paragraph_type == "model"], 
       y = df_aggregated$writer_confidence[df_aggregated$paragraph_type == "writer"], 
       mu = 0, conf.level = 0.95, paired = TRUE)

```

```
Paired t-tests leverage within-unit comparisons to reduce variability, leading to narrower confidence intervals and smaller p-values.

```









